{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930fb631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 09:55:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"FirstSparkSessionApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27ade09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('json').load('data/2015-summary.json')#, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83a4da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c819739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b68f9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad671bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891133d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|         Anguilla|\n",
      "|           Russia|\n",
      "|         Paraguay|\n",
      "|          Senegal|\n",
      "|           Sweden|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup= df.select('DEST_COUNTRY_NAME').dropDuplicates()\n",
    "df_dup.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3d63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"mobility_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2821d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0c2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|             Algeria|      United States|    4|\n",
      "|              Angola|      United States|   15|\n",
      "|            Anguilla|      United States|   41|\n",
      "| Antigua and Barbuda|      United States|  126|\n",
      "|           Argentina|      United States|  180|\n",
      "|               Aruba|      United States|  346|\n",
      "|           Australia|      United States|  329|\n",
      "|             Austria|      United States|   62|\n",
      "|          Azerbaijan|      United States|   21|\n",
      "|             Bahrain|      United States|   19|\n",
      "|            Barbados|      United States|  154|\n",
      "|             Belgium|      United States|  259|\n",
      "|              Belize|      United States|  188|\n",
      "|             Bermuda|      United States|  183|\n",
      "|             Bolivia|      United States|   30|\n",
      "|Bonaire, Sint Eus...|      United States|   58|\n",
      "|              Brazil|      United States|  853|\n",
      "|British Virgin Is...|      United States|  107|\n",
      "|            Bulgaria|      United States|    3|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('DEST_COUNTRY_NAME').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350dfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "380b07d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn('withInCountry', expr('ORIGIN_COUNTRY_NAME==DEST_COUNTRY_NAME'))\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c96ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:=============================================>         (83 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|withinCountry|count|\n",
      "+-------------+-----+\n",
      "|         true|    1|\n",
      "|        false|  255|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"withinCountry\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7fad9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|    United States|            Romania|   15|        false|   upper|\n",
      "|    United States|            Croatia|    1|        false|   under|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL 구문 CASE WHEN > 수치형 변수 > 명목형 변수로 변환\n",
    "df4 = df3.withColumn('category', expr('CASE WHEN count<10 THEN \"under\" WHEN count>=10 THEN \"upper\" END '))\n",
    "df4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a8c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|   under|   48|\n",
      "|   upper|  208|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupBy(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2173449b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|    United States|            Romania|   30|        false|   upper|\n",
      "|    United States|            Croatia|    2|        false|   under|\n",
      "|    United States|            Ireland|  688|        false|   upper|\n",
      "|            Egypt|      United States|   30|        false|   upper|\n",
      "|    United States|              India|  124|        false|   upper|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count 2배 해보기\n",
    "df5 = df4.withColumn('count', expr('count * 2'))\n",
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4edc67e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|       United States|            Croatia|    2|        false|   under|\n",
      "|       United States|          Singapore|    2|        false|   under|\n",
      "|             Moldova|      United States|    2|        false|   under|\n",
      "|               Malta|      United States|    2|        false|   under|\n",
      "|       United States|          Gibraltar|    2|        false|   under|\n",
      "|Saint Vincent and...|      United States|    2|        false|   under|\n",
      "|            Suriname|      United States|    2|        false|   under|\n",
      "|       United States|             Cyprus|    2|        false|   under|\n",
      "|             Liberia|      United States|    4|        false|   under|\n",
      "|             Hungary|      United States|    4|        false|   under|\n",
      "|       United States|            Vietnam|    4|        false|   under|\n",
      "|        Burkina Faso|      United States|    2|        false|   under|\n",
      "|            Djibouti|      United States|    2|        false|   under|\n",
      "|       United States|            Estonia|    2|        false|   under|\n",
      "|              Zambia|      United States|    2|        false|   under|\n",
      "|            Malaysia|      United States|    4|        false|   under|\n",
      "|             Croatia|      United States|    4|        false|   under|\n",
      "|              Cyprus|      United States|    2|        false|   under|\n",
      "|       United States|            Liberia|    4|        false|   under|\n",
      "|       United States|              Malta|    4|        false|   under|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df5.where('count<5').show()\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f149ee",
   "metadata": {},
   "source": [
    "## Projection 과 Filter\n",
    "\n",
    "select a,b,c from TableA # protection > column >Transforamtion /\n",
    "where a>10 #filter > Row > Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f280f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|       United States|            Croatia|    2|        false|   under|\n",
      "|       United States|          Singapore|    2|        false|   under|\n",
      "|             Moldova|      United States|    2|        false|   under|\n",
      "|               Malta|      United States|    2|        false|   under|\n",
      "|       United States|          Gibraltar|    2|        false|   under|\n",
      "|Saint Vincent and...|      United States|    2|        false|   under|\n",
      "|            Suriname|      United States|    2|        false|   under|\n",
      "|       United States|             Cyprus|    2|        false|   under|\n",
      "|             Liberia|      United States|    4|        false|   under|\n",
      "|             Hungary|      United States|    4|        false|   under|\n",
      "|       United States|            Vietnam|    4|        false|   under|\n",
      "|        Burkina Faso|      United States|    2|        false|   under|\n",
      "|            Djibouti|      United States|    2|        false|   under|\n",
      "|       United States|            Estonia|    2|        false|   under|\n",
      "|              Zambia|      United States|    2|        false|   under|\n",
      "|            Malaysia|      United States|    4|        false|   under|\n",
      "|             Croatia|      United States|    4|        false|   under|\n",
      "|              Cyprus|      United States|    2|        false|   under|\n",
      "|       United States|            Liberia|    4|        false|   under|\n",
      "|       United States|              Malta|    4|        false|   under|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df5.where('count<5').show()\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5923bff1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|    United States|            Croatia|    2|        false|   under|\n",
      "|    United States|          Singapore|    2|        false|   under|\n",
      "|    United States|          Gibraltar|    2|        false|   under|\n",
      "|    United States|             Cyprus|    2|        false|   under|\n",
      "|    United States|            Vietnam|    4|        false|   under|\n",
      "|    United States|            Estonia|    2|        false|   under|\n",
      "|    United States|            Liberia|    4|        false|   under|\n",
      "|    United States|              Malta|    4|        false|   under|\n",
      "|    United States|          Lithuania|    2|        false|   under|\n",
      "|    United States|           Bulgaria|    2|        false|   under|\n",
      "|    United States|            Georgia|    2|        false|   under|\n",
      "|    United States|            Bahrain|    2|        false|   under|\n",
      "|    United States|   Papua New Guinea|    2|        false|   under|\n",
      "|    United States|          Indonesia|    4|        false|   under|\n",
      "|    United States|         Montenegro|    2|        false|   under|\n",
      "|    United States|            Namibia|    2|        false|   under|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df5.where('count<5').where('ORIGIN_COUNTRY_NAME != \"United States\"')\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73591be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4a33052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----+-------------+--------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+------------------+-------------------+-----+-------------+--------+\n",
      "|            Canada|      United States|16798|        false|   upper|\n",
      "|            Mexico|      United States|14280|        false|   upper|\n",
      "|    United Kingdom|      United States| 4050|        false|   upper|\n",
      "|             Japan|      United States| 3096|        false|   upper|\n",
      "|           Germany|      United States| 2936|        false|   upper|\n",
      "|Dominican Republic|      United States| 2706|        false|   upper|\n",
      "|       South Korea|      United States| 2096|        false|   upper|\n",
      "|       The Bahamas|      United States| 1910|        false|   upper|\n",
      "|            France|      United States| 1870|        false|   upper|\n",
      "|          Colombia|      United States| 1746|        false|   upper|\n",
      "+------------------+-------------------+-----+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 국내여행 \n",
    "df10 = df5.where('DEST_COUNTRY_NAME != \"United States\"').orderBy(F.desc('count'))\n",
    "df10.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c78a9383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|category|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "|              Zambia|      United States|    2|        false|   under|\n",
      "|             Moldova|      United States|    2|        false|   under|\n",
      "|               Malta|      United States|    2|        false|   under|\n",
      "|            Djibouti|      United States|    2|        false|   under|\n",
      "|Saint Vincent and...|      United States|    2|        false|   under|\n",
      "|        Burkina Faso|      United States|    2|        false|   under|\n",
      "|            Suriname|      United States|    2|        false|   under|\n",
      "|              Cyprus|      United States|    2|        false|   under|\n",
      "|       Cote d'Ivoire|      United States|    2|        false|   under|\n",
      "|              Kosovo|      United States|    2|        false|   under|\n",
      "+--------------------+-------------------+-----+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df5.where('DEST_COUNTRY_NAME != \"United States\"').orderBy('count')\n",
    "df9.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f688d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|withInCountry|category|\n",
      "+-----------------+-------------------+------+-------------+--------+\n",
      "|    United States|      United States|740004|         true|   upper|\n",
      "+-----------------+-------------------+------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8= df5.where('count>5').where('withInCountry == true')\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b84df",
   "metadata": {},
   "source": [
    "## 집계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1512bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SecondSparkSessionApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f35674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('data/emp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2922745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "652e8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|null|    70|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62665a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| ename|deptno|\n",
      "+------+------+\n",
      "| SMITH|    20|\n",
      "| ALLEN|    30|\n",
      "|  WARD|    30|\n",
      "| JONES|    20|\n",
      "|MARTIN|    30|\n",
      "| BLAKE|    30|\n",
      "| CLARK|    10|\n",
      "| SCOTT|    20|\n",
      "|  KING|    10|\n",
      "|TURNER|    30|\n",
      "| ADAMS|    20|\n",
      "| JAMES|    30|\n",
      "|  FORD|    20|\n",
      "|MILLER|    10|\n",
      "|  JACK|    70|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ename', 'deptno').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd94e451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|ename|deptno|\n",
      "+-----+------+\n",
      "|SMITH|    20|\n",
      "|JONES|    20|\n",
      "|SCOTT|    20|\n",
      "|ADAMS|    20|\n",
      "| FORD|    20|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ename', 'deptno').where('deptno=20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db8bebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카운트 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b801d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc4e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(job)|\n",
      "+----------+\n",
      "|        15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b837cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('count(*)').show() # null 값 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8db89352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|  ANALYST|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|  MANAGER|\n",
      "|PRESIDENT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10b3a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('job').distinct().count() #정확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50da3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80d6ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 64:====================================>                 (137 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT job)|\n",
      "+-------------------+\n",
      "|                  5|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 64:===============================================>      (177 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('job')).show() # 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47a75564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(job)|\n",
      "+--------------------------+\n",
      "|                         5|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct('job', 0.1)).show() # 성능면에서 유리한 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc68043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first,last,min,max,sum,avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e737ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab6377b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min(sal)|max(sal)|\n",
      "+--------+--------+\n",
      "|     800|    5000|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min('sal'), max('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a12bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+----------+\n",
      "|count(empno)|count(1)|max(ename)|min(ename)|\n",
      "+------------+--------+----------+----------+\n",
      "|          15|      15|      WARD|     ADAMS|\n",
      "+------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count('empno'), count('*'), max('ename'), min('ename')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c9ff15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(sal)|\n",
      "+--------+\n",
      "|   32225|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum('sal')).show() # sal 컬럼의 총합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cceebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:========================================>             (150 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|sum(DISTINCT sal)|\n",
      "+-----------------+\n",
      "|            27975|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#sal 컬럼값의 중복을 제거하고 합산\n",
    "#df.select().distinct().sum()\n",
    "df.selectExpr('sum( distinct sal)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d18c2e",
   "metadata": {},
   "source": [
    "alias() 함수는 PySpark에서 DataFrame의 컬럼에 새로운 이름을 부여하는 데 사용되는 중요한 메서드입니다. \\\n",
    "이 함수는 주로 다음과 같은 상황에서 활용됩니다: \\\n",
    "사용 목적 \\\n",
    "컬럼 이름 변경: 기존 컬럼에 더 의미 있거나 읽기 쉬운 이름을 부여할 때 사용됩니다. \\\n",
    "집계 결과 이름 지정: groupBy() 연산 후 집계 함수의 결과에 이름을 부여할 때 특히 유용합니다 \\\n",
    "표현식 결과 이름 지정: 복잡한 연산이나 함수의 결과를 새로운 컬럼으로 만들 때 사용됩니다. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2625b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+------------------+\n",
      "|total_tx|total_salary|        avg_salary|       mean_salary|\n",
      "+--------+------------+------------------+------------------+\n",
      "|      15|       32225|2148.3333333333335|2148.3333333333335|\n",
      "+--------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alias\n",
    "dfs = df.select( count('sal').alias('total_tx'),\n",
    "                    sum('sal').alias('total_salary'),\n",
    "                    avg('sal').alias('avg_salary'),\n",
    "                    expr('mean(sal)').alias('mean_salary')\n",
    "               )\n",
    "\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74978f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|rounded_avg_salary|\n",
      "+------------------+\n",
      "|           2148.33|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# round(data, 자릿수)\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.select(round(avg('sal'), 2).alias('rounded_avg_salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357645b",
   "metadata": {},
   "source": [
    "## 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b280bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "|  ANALYST|    2|\n",
      "| SALESMAN|    4|\n",
      "|    CLERK|    5|\n",
      "|  MANAGER|    3|\n",
      "|PRESIDENT|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('job').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d328d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|      job|           SAL_AVG|\n",
      "+---------+------------------+\n",
      "|  ANALYST|            3000.0|\n",
      "| SALESMAN|            1400.0|\n",
      "|    CLERK|            1470.0|\n",
      "|  MANAGER|2758.3333333333335|\n",
      "|PRESIDENT|            5000.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#agg() 집계 함수 적용\n",
    "dfs = df.groupby('job').agg( expr('avg(sal) as SAL_AVG'))\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11b4b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sal 표준편차 = 데이터의 분산 정도를 나타내는 통계적 측정값\n",
    "#1. sal.function stddev()\n",
    "#모집단 표준편차(stddev_pop())와 표본 표준편차(stddev_samp())\n",
    "#2. sql expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09947ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|      job|         SAL_STDEV|\n",
      "+---------+------------------+\n",
      "|  ANALYST|               0.0|\n",
      "| SALESMAN|154.11035007422439|\n",
      "|    CLERK| 880.6815542521599|\n",
      "|  MANAGER|223.91714737574006|\n",
      "|PRESIDENT|               0.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('job').agg( expr('stddev_pop(sal) as SAL_STDEV')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d996b",
   "metadata": {},
   "source": [
    "## 윈도우함수\n",
    "\n",
    "순위,정렬 -rank,row_number,dense_rank 누계 - sum,avg,max,min + over() 이동평균, 이동합계 - over + \n",
    "rowsBetween, rangeBetween 시차,선행 - lag, lead\n",
    "\n",
    "ex) 세션 구간 내 분석, 특정시간 동안 일어난 활동 그룹화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7a0c3",
   "metadata": {},
   "source": [
    "1. partitionBy() > 소그룹으로 나눈다.\n",
    "2. orderBy() > 소그룹 내 정렬\n",
    "3. rowBetween(), rangeBetween()\n",
    "4. over()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4aba5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc,rank\n",
    "\n",
    "# 순위를 부여하려고 하는 데이터 범위 > 윈도우 명세 설정\n",
    "windowspec = Window.orderBy(desc('sal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2d2f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'RANK() OVER (ORDER BY sal DESC NULLS LAST unspecifiedframe$())'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#순위객체\n",
    "salAllRank = rank().over(windowspec)\n",
    "salAllRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "640fd6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 10:32:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|salary_rank|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|          1|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|null|    70|          2|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|null|    20|          3|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|null|    20|          3|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|null|    20|          5|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|          6|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|          7|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|          8|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|          9|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|         10|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|         11|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|         11|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|null|    20|         13|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|         14|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|         15|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터프레임에 컬럼으로 추가 > 액션\n",
    "df.withColumn(\"salary_rank\", salAllRank).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "704b76e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|empno|salary_rank|\n",
      "+-----+-----------+\n",
      "| 7839|          1|\n",
      "| 9292|          2|\n",
      "| 7788|          3|\n",
      "| 7902|          3|\n",
      "| 7566|          5|\n",
      "| 7698|          6|\n",
      "| 7782|          7|\n",
      "| 7499|          8|\n",
      "| 7844|          9|\n",
      "| 7934|         10|\n",
      "| 7521|         11|\n",
      "| 7654|         11|\n",
      "| 7876|         13|\n",
      "| 7900|         14|\n",
      "| 7369|         15|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 10:33:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select( 'empno', salAllRank.alias('salary_rank')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d0a1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#windowspec = Window.orderBy(desc('sal'))\n",
    "windowspec1 = Window.partitionBy('job').orderBy(desc('sal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7b37e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "salJobRank = rank().over(windowspec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bad2215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----+----------+\n",
      "|      job| ename| sal|salJobRank|\n",
      "+---------+------+----+----------+\n",
      "|  ANALYST| SCOTT|3000|         1|\n",
      "|  ANALYST|  FORD|3000|         1|\n",
      "| SALESMAN| ALLEN|1600|         1|\n",
      "| SALESMAN|TURNER|1500|         2|\n",
      "| SALESMAN|  WARD|1250|         3|\n",
      "| SALESMAN|MARTIN|1250|         3|\n",
      "|    CLERK|  JACK|3200|         1|\n",
      "|    CLERK|MILLER|1300|         2|\n",
      "|    CLERK| ADAMS|1100|         3|\n",
      "|    CLERK| JAMES| 950|         4|\n",
      "|    CLERK| SMITH| 800|         5|\n",
      "|  MANAGER| JONES|2975|         1|\n",
      "|  MANAGER| BLAKE|2850|         2|\n",
      "|  MANAGER| CLARK|2450|         3|\n",
      "|PRESIDENT|  KING|5000|         1|\n",
      "+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    'job', 'ename', 'sal',\n",
    "    salJobRank.alias('salJobRank')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6f8e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#부서별 급여 순위 생성\n",
    "dept_window_spec = Window.partitionBy('deptno').orderBy(desc('sal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8eb51be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+----------------+\n",
      "| ename|deptno| sal|dept_salary_rank|\n",
      "+------+------+----+----------------+\n",
      "| SCOTT|    20|3000|               1|\n",
      "|  FORD|    20|3000|               1|\n",
      "| JONES|    20|2975|               3|\n",
      "| ADAMS|    20|1100|               4|\n",
      "| SMITH|    20| 800|               5|\n",
      "|  KING|    10|5000|               1|\n",
      "| CLARK|    10|2450|               2|\n",
      "|MILLER|    10|1300|               3|\n",
      "|  JACK|    70|3200|               1|\n",
      "| BLAKE|    30|2850|               1|\n",
      "| ALLEN|    30|1600|               2|\n",
      "|TURNER|    30|1500|               3|\n",
      "|  WARD|    30|1250|               4|\n",
      "|MARTIN|    30|1250|               4|\n",
      "| JAMES|    30| 950|               6|\n",
      "+------+------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn( 'dept_salary_rank', rank().over(dept_window_spec))\n",
    "new_df.select(\n",
    "    'ename', 'deptno', 'sal', 'dept_salary_rank'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b417e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+----------------+\n",
      "| ename|deptno| sal|dept_salary_rank|\n",
      "+------+------+----+----------------+\n",
      "| SCOTT|    20|3000|               1|\n",
      "|  FORD|    20|3000|               2|\n",
      "| JONES|    20|2975|               3|\n",
      "| ADAMS|    20|1100|               4|\n",
      "| SMITH|    20| 800|               5|\n",
      "|  KING|    10|5000|               1|\n",
      "| CLARK|    10|2450|               2|\n",
      "|MILLER|    10|1300|               3|\n",
      "|  JACK|    70|3200|               1|\n",
      "| BLAKE|    30|2850|               1|\n",
      "| ALLEN|    30|1600|               2|\n",
      "|TURNER|    30|1500|               3|\n",
      "|  WARD|    30|1250|               4|\n",
      "|MARTIN|    30|1250|               5|\n",
      "| JAMES|    30| 950|               6|\n",
      "+------+------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "new_df = df.withColumn( 'dept_salary_rank', row_number().over(dept_window_spec))\n",
    "new_df.select(\n",
    "    'ename', 'deptno', 'sal', 'dept_salary_rank'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30fdcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#누적 급여 계산\n",
    "from pyspark.sql.functions import dense_rank\n",
    "sum_window_spec = Window.partitionBy('deptno').orderBy('empno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed65ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+----------+\n",
      "| ename|deptno| sal|cum_salary|\n",
      "+------+------+----+----------+\n",
      "| SMITH|    20| 800|       800|\n",
      "| JONES|    20|2975|      3775|\n",
      "| SCOTT|    20|3000|      6775|\n",
      "| ADAMS|    20|1100|      7875|\n",
      "|  FORD|    20|3000|     10875|\n",
      "| CLARK|    10|2450|      2450|\n",
      "|  KING|    10|5000|      7450|\n",
      "|MILLER|    10|1300|      8750|\n",
      "|  JACK|    70|3200|      3200|\n",
      "| ALLEN|    30|1600|      1600|\n",
      "|  WARD|    30|1250|      2850|\n",
      "|MARTIN|    30|1250|      4100|\n",
      "| BLAKE|    30|2850|      6950|\n",
      "|TURNER|    30|1500|      8450|\n",
      "| JAMES|    30| 950|      9400|\n",
      "+------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn( 'cum_salary', sum('sal').over(sum_window_spec))\n",
    "new_df.select(\n",
    "    'ename', 'deptno', 'sal', 'cum_salary'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#부서별 평균급여 avg('sal')\n",
    "#sql select 문\n",
    "\n",
    "    SELECT #projection\n",
    "    ename, deptno, sal,\n",
    "    avg('sal').over(partition by deptno) as dept_avg_salary\n",
    "    from emp; #filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da31babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:=================================================>    (92 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+\n",
      "|empno|deptno|   dept_avg_salary|\n",
      "+-----+------+------------------+\n",
      "| 7369|    20|            2175.0|\n",
      "| 7566|    20|            2175.0|\n",
      "| 7788|    20|            2175.0|\n",
      "| 7876|    20|            2175.0|\n",
      "| 7902|    20|            2175.0|\n",
      "| 7782|    10|2916.6666666666665|\n",
      "| 7839|    10|2916.6666666666665|\n",
      "| 7934|    10|2916.6666666666665|\n",
      "| 9292|    70|            3200.0|\n",
      "| 7499|    30|1566.6666666666667|\n",
      "| 7521|    30|1566.6666666666667|\n",
      "| 7654|    30|1566.6666666666667|\n",
      "| 7698|    30|1566.6666666666667|\n",
      "| 7844|    30|1566.6666666666667|\n",
      "| 7900|    30|1566.6666666666667|\n",
      "+-----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "avg_window_spec = Window.partitionBy('deptno')\n",
    "new_avg_df = df.withColumn('dept_avg_salary', avg('sal').over(avg_window_spec))\n",
    "new_avg_df.select('empno', 'deptno', 'dept_avg_salary',).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08d38e",
   "metadata": {},
   "source": [
    "    select \n",
    "    ename, deptno, sal,\n",
    "    LAG(sal)  OVER( partition by deptno order by empno ) as prev_salary\n",
    "    LEAD(sal) OVER( partition by deptno order by empno ) as next_salary\n",
    "    from emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c1f84fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----------+-----------+\n",
      "| ename|deptno| sal|prev_salary|next_salary|\n",
      "+------+------+----+-----------+-----------+\n",
      "| SMITH|    20| 800|       null|       2975|\n",
      "| JONES|    20|2975|        800|       3000|\n",
      "| SCOTT|    20|3000|       2975|       1100|\n",
      "| ADAMS|    20|1100|       3000|       3000|\n",
      "|  FORD|    20|3000|       1100|       null|\n",
      "| CLARK|    10|2450|       null|       5000|\n",
      "|  KING|    10|5000|       2450|       1300|\n",
      "|MILLER|    10|1300|       5000|       null|\n",
      "|  JACK|    70|3200|       null|       null|\n",
      "| ALLEN|    30|1600|       null|       1250|\n",
      "|  WARD|    30|1250|       1600|       1250|\n",
      "|MARTIN|    30|1250|       1250|       2850|\n",
      "| BLAKE|    30|2850|       1250|       1500|\n",
      "|TURNER|    30|1500|       2850|        950|\n",
      "| JAMES|    30| 950|       1500|       null|\n",
      "+------+------+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lead, lag 이전 급여 , 이후 급여\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "row_window_spec = Window.partitionBy('deptno').orderBy('empno')\n",
    "\n",
    "#이전급여 컬럼, 이후급여 컬럼 2개 추가\n",
    "lead_lagg_sal_df = df.withColumn('prev_salary', lag('sal').over(row_window_spec))\\\n",
    "            .withColumn('next_salary', lead('sal').over(row_window_spec))\n",
    "\n",
    "lead_lagg_sal_df.select('ename', 'deptno', 'sal',  'prev_salary', 'next_salary').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964e53c",
   "metadata": {},
   "source": [
    "# rollup, cube\n",
    "\n",
    "over(), groupby()\\\n",
    "rollup : 계층적집계, 부분합(subtotal), 총합(grandtotal)\\\n",
    "cube : 모든 값으로 부분합 , 결합 가능합 모든 값의 부분합을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cab3ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `attachment:c3d0c6ad-29c8-4d6c-81c3-360a6ac841ad.png'\r\n",
      "/bin/bash: -c: line 0: `[image.png](attachment:c3d0c6ad-29c8-4d6c-81c3-360a6ac841ad.png)'\r\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:c3d0c6ad-29c8-4d6c-81c3-360a6ac841ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8255b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `attachment:6f184329-7cd3-4563-aca8-a31ac7a57fa2.png'\r\n",
      "/bin/bash: -c: line 0: `[image.png](attachment:6f184329-7cd3-4563-aca8-a31ac7a57fa2.png)'\r\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:6f184329-7cd3-4563-aca8-a31ac7a57fa2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb9e3a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: string, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd37acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 177:==================================>                  (132 + 2) / 200]\r",
      "\r",
      "[Stage 177:================================================>    (183 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+\n",
      "|deptno|      job|count(1)|sum(sal)|\n",
      "+------+---------+--------+--------+\n",
      "|    10|    CLERK|       1|    1300|\n",
      "|    10|  MANAGER|       1|    2450|\n",
      "|    10|PRESIDENT|       1|    5000|\n",
      "|    20|  ANALYST|       2|    6000|\n",
      "|    20|    CLERK|       2|    1900|\n",
      "|    20|  MANAGER|       1|    2975|\n",
      "|    30|    CLERK|       1|     950|\n",
      "|    30|  MANAGER|       1|    2850|\n",
      "|    30| SALESMAN|       4|    5600|\n",
      "|    70|    CLERK|       1|    3200|\n",
      "+------+---------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 그룹화 > 소계\n",
    "\n",
    "df.groupBy('deptno', 'job').agg(count('*'),sum('sal')).orderBy('deptno', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ef3785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 181:=====================================>               (140 + 2) / 200]\r",
      "\r",
      "[Stage 181:====================================================>(197 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+\n",
      "|deptno|      job|count(1)|sum(sal)|\n",
      "+------+---------+--------+--------+\n",
      "|  null|     null|      15|   32225|\n",
      "|  null|  ANALYST|       2|    6000|\n",
      "|  null|    CLERK|       5|    7350|\n",
      "|  null|  MANAGER|       3|    8275|\n",
      "|  null|PRESIDENT|       1|    5000|\n",
      "|  null| SALESMAN|       4|    5600|\n",
      "|    10|     null|       3|    8750|\n",
      "|    10|    CLERK|       1|    1300|\n",
      "|    10|  MANAGER|       1|    2450|\n",
      "|    10|PRESIDENT|       1|    5000|\n",
      "|    20|     null|       5|   10875|\n",
      "|    20|  ANALYST|       2|    6000|\n",
      "|    20|    CLERK|       2|    1900|\n",
      "|    20|  MANAGER|       1|    2975|\n",
      "|    30|     null|       6|    9400|\n",
      "|    30|    CLERK|       1|     950|\n",
      "|    30|  MANAGER|       1|    2850|\n",
      "|    30| SALESMAN|       4|    5600|\n",
      "|    70|     null|       1|    3200|\n",
      "|    70|    CLERK|       1|    3200|\n",
      "+------+---------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#null 값 포함\n",
    "df.cube('deptno', 'job').agg(count('*'),sum('sal')).orderBy('deptno', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28fe6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롤업 -> 대,중분류 최대.최소값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33043f",
   "metadata": {},
   "source": [
    "```\n",
    "select\n",
    "    deptno, job,\n",
    "    max(sal) as max_sal,\n",
    "    min(sal) as min_sal,\n",
    "from emp\n",
    "group by rollup(deptno, job)\n",
    "order by deptno, job;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d14b1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+-------+\n",
      "|deptno|      job|max_sal|min_sal|\n",
      "+------+---------+-------+-------+\n",
      "|  null|     null|   5000|    800|\n",
      "|    10|     null|   5000|   1300|\n",
      "|    10|    CLERK|   1300|   1300|\n",
      "|    10|  MANAGER|   2450|   2450|\n",
      "|    10|PRESIDENT|   5000|   5000|\n",
      "|    20|     null|   3000|    800|\n",
      "|    20|  ANALYST|   3000|   3000|\n",
      "|    20|    CLERK|   1100|    800|\n",
      "|    20|  MANAGER|   2975|   2975|\n",
      "|    30|     null|   2850|    950|\n",
      "|    30|    CLERK|    950|    950|\n",
      "|    30|  MANAGER|   2850|   2850|\n",
      "|    30| SALESMAN|   1600|   1250|\n",
      "|    70|     null|   3200|   3200|\n",
      "|    70|    CLERK|   3200|   3200|\n",
      "+------+---------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 183:========================================>            (154 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.rollup('deptno', 'job').agg( max('sal').alias('max_sal'), min('sal').alias('min_sal') ).orderBy('deptno', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7445b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job 별 평균급여 job, avg_salary, total_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c797c348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------+\n",
      "|      job|           avg_sal|total_sal|\n",
      "+---------+------------------+---------+\n",
      "|     null|2148.3333333333335|    32225|\n",
      "|  ANALYST|            3000.0|     6000|\n",
      "|    CLERK|            1470.0|     7350|\n",
      "|  MANAGER|2758.3333333333335|     8275|\n",
      "|PRESIDENT|            5000.0|     5000|\n",
      "| SALESMAN|            1400.0|     5600|\n",
      "+---------+------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 185:================================================>    (182 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.rollup('job').agg(avg('sal').alias('avg_sal'), sum('sal').alias('total_sal')).orderBy('job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91840709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dpetno, job 평균급여, 최대급여를 모두 소계를 냅니다. -cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fff5eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 187:===================================>                 (133 + 2) / 200]\r",
      "\r",
      "[Stage 187:=================================================>   (186 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+-------+\n",
      "|deptno|      job|avg_sal|max_sal|\n",
      "+------+---------+-------+-------+\n",
      "|  null|     null|2148.33|   5000|\n",
      "|  null|  ANALYST| 3000.0|   3000|\n",
      "|  null|    CLERK| 1470.0|   3200|\n",
      "|  null|  MANAGER|2758.33|   2975|\n",
      "|  null|PRESIDENT| 5000.0|   5000|\n",
      "|  null| SALESMAN| 1400.0|   1600|\n",
      "|    10|     null|2916.67|   5000|\n",
      "|    10|    CLERK| 1300.0|   1300|\n",
      "|    10|  MANAGER| 2450.0|   2450|\n",
      "|    10|PRESIDENT| 5000.0|   5000|\n",
      "|    20|     null| 2175.0|   3000|\n",
      "|    20|  ANALYST| 3000.0|   3000|\n",
      "|    20|    CLERK|  950.0|   1100|\n",
      "|    20|  MANAGER| 2975.0|   2975|\n",
      "|    30|     null|1566.67|   2850|\n",
      "|    30|    CLERK|  950.0|    950|\n",
      "|    30|  MANAGER| 2850.0|   2850|\n",
      "|    30| SALESMAN| 1400.0|   1600|\n",
      "|    70|     null| 3200.0|   3200|\n",
      "|    70|    CLERK| 3200.0|   3200|\n",
      "+------+---------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "df.cube('deptno', 'job')\\\n",
    "    .agg( \\\n",
    "        count('*'),  \n",
    "        round(avg('sal'),2).alias('avg_sal'),\n",
    "        max('sal').alias('max_sal'))\\\n",
    "    .orderBy('deptno', 'job')\\\n",
    "    .select('deptno', 'job', 'avg_sal', 'max_sal')\\\n",
    "    .show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "19997f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad21c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
